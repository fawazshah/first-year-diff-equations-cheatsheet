\documentclass{article}
\title{M1J2 Summary Notes (JMC Year 1, 2017/2018 syllabus)}
\date{}
\author{Fawaz Shah (original notes by Dr Marie-Amelie Lawn)}

% Packages for adding hyperlinks to table of contents
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}

%package that allows aligned equations
\usepackage{amsmath}

%package that allows notation for extra mathematical symbols
\usepackage{amssymb}

%new commands for popular sets for ease of use
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

% renaming command to writing vectors in bold notation
\renewcommand{\vec}[1]{\mathbf{#1}}

%package for managing images
\usepackage{graphicx}
\graphicspath{ {img/} }

%package for managing hyperlinks
\usepackage{hyperref}

\begin{document}
\large
\maketitle
\begin{center}
(STILL UNDER CONSTRUCTION)
\end{center}
\noindent Dr Lawn refers to propositions, theorems, corollaries and lemmas. In this document I will refer to them all as 'theorems'.
\\\\
\noindent This document contains a list of definitions and a list of theorems.
\tableofcontents
\newpage
\part{Abstract Linear Algebra}

\section{Definitions}
\paragraph{Vector space}
A vector space is a set $ V $ coupled with:
\begin{itemize}
\item a function $ + : V \times V \to V $ (addition)
\item a function $ \cdot : \R \times V \to V $ (scalar multiplication)
\end{itemize}
From now on we will denote $ V $ as an arbitrary vector space.
\paragraph{Subspace}
A subset $ U \subseteq V $ is a subspace if:
\begin{itemize}
\item $ 0_{V} \in U $
\item If $ \vec{x}, \vec{y} \in U $ then $ \vec{x + y} \in U $ (closure under addition)
\item If $ \vec{x} \in U $ then for all $ \lambda \in \R $, $ \lambda\vec{x} \in U $ (closure under scalar multiplication)
\end{itemize}
\paragraph{Linear combination}
A linear combination of a set of vectors $ \{\vec{v}_{1}...\vec{v}_{n}\} $ is any vector $ \vec{x} $ of the form:
\begin{equation}
\vec{x} = \lambda_{1}\vec{v}_{1} + \lambda_{2}\vec{v}_{2} + ... + \lambda_{n}\vec{v}_{n}
\end{equation}
for some real numbers $ \lambda_{1}...\lambda_{n} $
\paragraph{Span}
The span of a set $ S \subseteq V $ is the set of all linear combinations of elements of $ S $. We define span$ (\emptyset) = \{0_{V}\} $.
\paragraph{Spanning set}
A subset $ S \subseteq V $ is called a spanning set of $ V $ if span$ (S) = V $.
\paragraph{Finite dimensional}
A vector space is finite dimensional if it contains a finite spanning set.
\paragraph{Linear dependence}
A subset of vectors $ \{ \vec{v}_{1}...\vec{v}_{n} \}  \subseteq V $ is linearly dependent if there exists some real numbers $ \lambda_{1}...\lambda_{n} $ (which are not just all $ 0 $s) such that:
\begin{equation}
\lambda_{1}\vec{v}_{1} + \lambda_{2}\vec{v}_{2} + ... + \lambda_{n}\vec{v}_{n} = 0
\end{equation}
\paragraph{Basis}
A basis of a vector space is a linearly independent spanning set.
\paragraph{Standard basis of $ \R^{n} $}
We define the standard basis elements of any $ \R^{n} $ to be:
\begin{equation}
e_{1} = 
\begin{pmatrix}
1 \\ 0 \\ \vdots \\ 0 \\ 0
\end{pmatrix} , \ 
e_{2} = 
\begin{pmatrix}
0 \\ 1 \\ \vdots \\ 0 \\ 0
\end{pmatrix} ... \
e_{n-1} = 
\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 1 \\ 0
\end{pmatrix} , \
e_{n} = 
\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0 \\ 1
\end{pmatrix}
\end{equation}
The standard basis of $ \R^{n} $ is therefore $ \{e_{1}, e_{2} \ ... \ e_{n} \} $.
\paragraph{Linear map}
Let $ U $ and $ V $ be vector spaces. A linear map is a function $ f : U \to V $ such that:
\begin{itemize}
\item for all $ \vec{x}, \vec{y} \in U, \ f(\vec{x} + \vec{y}) = f(\vec{x}) + f(\vec{y}) $
\item for all $ \vec{x} \in U $ and $ \lambda \in \R, \ f(\lambda \vec{x}) = \lambda f(\vec{x}) $
\end{itemize}
\paragraph{Image}
The image of a linear map $ f : U \to V $ is the set of all $ f(\vec{u}) \in V $ where $ \vec{u} \in U $.
\begin{equation}
\textrm{image}(f) = \{f(\vec{u}) \ | \ u \in U \}
\end{equation}
\paragraph{Kernel}
The kernel of a linear map $ f : U \to V $ is the set of all $ \vec{u} \in U $ such that $ f(\vec{u}) = 0_{V} $.
\begin{equation}
\textrm{kernel}(f) = \{\vec{u} \ | \ u \in U, f(\vec{u}) = 0_{V} \}
\end{equation}
\paragraph{Isomorphism}
A linear map $ f : U \to V $ is an isomorphism if it is bijective. We say $ U \simeq V $.
\paragraph{Rank}
The rank of $ f $ is defined as dim(image($ f $)).
\paragraph{Nullity}
The rank of $ f $ is defined as dim(kernel($ f $)).
\paragraph{$ T_{A} $}
We define a function $ T_{A} $ that pre-multiplies a vector by a matrix $ A $:
\begin{equation}
T_{A} : \R^{n} \to \R^{m}, \ \vec{v} \mapsto \vec{A}\vec{v}, \ A \in \textrm{Mat}_{m \times n}(\R)
\end{equation}
Note that if $ A $ is an $ m \times n $ matrix, then $ T_{A} $ transforms a vector in $ \R^{n} $ to a vector in $ \R^{m} $.

\section{Theorems}

\subsection{Vector spaces}
\textit{Vector space axioms}
\begin{itemize}
\item $ (V, +) $ is an Abelian group (the identity element being $ 0_{V} $)
\item for any $ \vec{v} \in V $, 1$ \vec{v} = \vec{v} $
\item for any $ \vec{v} \in V, \lambda, \mu \in \R, \lambda (\mu \vec{v}) = (\lambda \mu) \vec{v} $ (commutative w.r.t. scalar multiplication)
\item for any $ \vec{u}, \vec{v} \in V, \lambda \in \R, \lambda (\vec{u}  + \vec{v}) = \lambda \vec{u}  + \lambda \vec{v} $ (scalar multiplication distributes over addition)
\item for any $ \vec{v} \in V, \lambda, \mu \in \R, (\lambda + \mu) \vec{v} = \lambda \vec{v}  + \mu \vec{v} $ (scalar multiplication distributes over scalar addition)
\end{itemize}
For any $ \vec{v} \in V $:
\begin{itemize}
\item For any $ n \in \Z $, $ n\vec{v} = \vec{v} + \vec{v} + ... + \vec{v} $ (n times)
\item $ 0 \vec{v} = 0_{V} $
\item $ (-1) \vec{v} $ is the additive inverse of $ \vec{v} $
\end{itemize}

\subsection{Subspaces}

\subsection{Spanning sets and linear independence}

\subsection{Bases and dimension}

\subsection{Linear maps}

\newpage
\part{Group Theory}

\section{Definitions}
\paragraph{Binary operation}
A binary operation on a set $ G $ is a any function $ f: G \times G \to G $
\paragraph{Associative}
A binary operation $ \star $ on a set $ G $ is associative if it satisfies:
\begin{equation}
(a \star b) \star c = a \star (b \star c)
\end{equation}
for all $ a, b, c \in G $.
\paragraph{Commutative}
A binary operation $ \star $ on a set $ G $ is commutative if it satisfies:
\begin{equation}
a \star b = b \star a
\end{equation}
for all $ a, b \in G $.
\paragraph{Left/right identity}
An element $ e \in G $ is called the left identity if:
\begin{equation}
e \star g = g
\end{equation}
for all $ g \in G $. Similar statement for right identity.
\paragraph{(Two sided) Identity element}
An element $ e \in G $ is a two-sided identity element if it is both a left identity and a right identity.
\\\\
From now on the two-sided identity element will be referred to as $ e $.
\paragraph{Left/right inverse}
An element $ h \in G $ is called the left inverse  of $ g \in G $ if:
\begin{equation}
h \star g = e
\end{equation}
Similar statement for right inverse.
\paragraph{Two sided inverse}
A two sided inverse of an element $ g \in G $ is both a left inverse and a right inverse of $ g $.
\\\\
From now on the two-sided inverse of $ g $ will be referred to as $ g^{-1} $.
\paragraph{Group}
A group $ (G, \star) $ is a set $ G $ equipped with a binary operation $ \star $ such that:
\begin{itemize}
\item $ \star $ is associative
\item $ \star $ has an identity element $ e \in G $
\item Every $ g \in G $ has an inverse $ g^{-1} \in G $
\end{itemize}
The above three suffice for the exam, however there is technically a fourth requirement:
\begin{itemize}
\item $ G $ is closed under $ \star $, i.e. for all $ g, h, \in G, g \star h \in G $
\end{itemize}
\paragraph{Order (group)}
The order of a group $ (G, \star) $ is the size of $ G $.
\paragraph{Abelian group}
An Abelian group is a group with a commutative binary operation $ \star $.
\paragraph{Powers of $ g $}
We can define the powers of any $ g \in G $ to be:
\begin{equation}
g^{n} = 
\begin{cases}
g \star g \star ... g & n > 0 \\
g^{-1} \star g^{-1} \star ... g^{-1} & n < 0 \\
e & n = 0
\end{cases}
\end{equation}
where in the first cases there are $ n $ copies of $ g $, and in the second case there are $ -n $ copies of $ g^{-1} $.
\paragraph{Definition of $ [a]_{n} $ and $ \Z_{n} $}
For any $ a \in \Z $:
\begin{equation}
[a]_{n} = \{b \in \Z \ | \ b \equiv a \textrm{ mod } n \}
\end{equation}
Note that $ [a]_{n} $ forms an equivalence class, and there are exactly $ n $ of these equivalence classes. $ \Z_{n} $ is the set of all these equivalence classes.
\begin{equation}
\Z_{n} = \{[a]_{n} \ | \ a \in \Z \}
\end{equation}
\paragraph{Definition of $ \Z_{n}^{*} $}
$ \Z_{n}^{*} $ is the set of all invertible $ [a]_{n} $. Note in this case the identity element is $ [1]_{n} $.
\begin{equation}
\Z_{n}^{*} = \{[a]_{n} \ | \ \exists [b]_{n} \in Z_{n} \quad s.t. \quad [a]_{n}[b]_{n} = [1]\}
\end{equation}
Note that $ [a]_{n}[b]_{n} = 1 \Leftrightarrow gcd(a, n) = 1 $.
\paragraph{Order (element)}
The order of any $ g \in G $ is the smallest positive integer such that:
\begin{equation}
g^{n} = e
\end{equation}
\paragraph{Cyclic group + generator}
A group $ (G, \star) $ is cyclic if:
\begin{equation}
G = \{ g^{n} \ | \ n \in \Z \} 
\end{equation}
$ g $ is called the generator of the group.
\paragraph{Permutation}
A permutation $ \sigma $ on $ n $ symbols is a bijection:
\begin{equation}
\sigma : \{1...n\} \to \{1...n\}
\end{equation}
\paragraph{Symmetric group}
The symmetric group $ S_{n} $ on $ n $ symbols is the set of all permutations of $ n $ symbols.
\begin{equation}
S_{n} = \{\sigma : \{1...n\} \to \{1...n\}\}
\end{equation}
Note that $ S_{n} $ is a set of functions. Therefore the identity element is the identity function.
\paragraph{$ k $-cycle}
A permutation $ \sigma \in S_{n} $ is a $ k $-cycle if there exists some $ a_{1}...a{k}  \in \{1...n\} $ such that:
\begin{equation}
\sigma(a_{1}) = a_{2}, \quad \sigma(a_{2}) = a_{3} \quad ... \quad \sigma(a_{k}) = a_{1}
\end{equation}
and $ \sigma(i) = i $ for all $ i \notin \{1...n\} $. $ k $ is called the length of the cycle. The notation for a cycle is $ (a_{1}...a_{k}) $.
\paragraph{Disjoint cycles}
Two cycles $ (a_{1}...a_{m}) $ and $ (b_{1}...b_{n}) $ are disjoint if no $ a_{i} $ is equal to any $ b_{j} $.
\paragraph{Subgroup}
Let $ (G, \star) $ be a group, and $ H \subseteq G $. $ (H, \star) $ is a subgroup of $ G $ if:
\begin{itemize}
\item $ e \in H $
\item For any $ g, h \in H $,  $ g \star h \in H $
\item For any $ g \in H $, $ g^{-1} \in H $
\end{itemize}
\paragraph{Cyclic subgroup}
Let $ (G, \star) $ be a group. For any $ g \in G $, the cyclic subgroup $ \langle g \rangle $ generated by $ g $ is defined as:
\begin{equation}
\langle g \rangle = (\{g^{i} \ | \ i \in \Z \}, \star)
\end{equation}
Note that order of $ g  = $  size of cyclic subgroup $ \langle g \rangle $.
\paragraph{Left/right cosets}
Let $ (G, \star) $ be a group and $ (H, \star) $ a subgroup. For any $ g \in G $, the left coset of $ H $ by $ g $ (denoted by $ gH $) is defined as:
\begin{equation}
gH = \{g \star h \ | \ h \in H \}
\end{equation}
Similar definition for right coset of $ H $ by $ g $ (denoted by $ Hg $).
\\\\
The set of all left cosets of $ H $ by $ g $ is denoted by $ G : H $.
\\
The set of all right cosets of $ H $ by $ g $ is denoted by $ H : G $.

\section{Theorems}

\subsection{Groups}
(For the rest of this section, assume $ (G, \star) $ is a group)
\\\\
Any identity element $ e $ is unique for that group.
\\\\
Any two-sided inverse $ g^{-1} $ of an element $ g \in G $ is unique.
\\\\
For any $g, h \in G $
\begin{equation}
(g \star h)^{-1} = h^{-1} \star g^{-1}
\end{equation}
The normal exponent rules apply within groups, e.g.
\begin{align}
g^{n} \star g^{m} & = g^{n+m} \\
(g^{n})^{-1} & = g^{-n} \\
(g^{n})^{m} & = g^{nm}
\end{align}
Some examples of groups:
($ \R, + $), ($ \Z, + $), ($ \Z^{*}, \times $)


\subsection{Modular arithmetic and $ \Z_{n} $}
($ \Z_{n}, + $) is an Abelian group.
\\\\
($ \Z_{n}^{*}, \ \cdot \ $) is an Abelian group.

\subsection{Cyclic groups}
If ($ G, \star $) is a finite group then every $ g \in G $ has finite order.
\\\\
Any $ g \in G $ with order n has distinct powers $ g^{0}, g^{1}, g^{2}...g^{n - 1} $.
\\\\
All cyclic groups are Abelian.
\\\\
Assume $ G $ is finite with size $ n $. \\
$ G $ is cyclic $ \Leftrightarrow G $ contains an element of order $ n $.
\\\\
\includegraphics[scale=0.4]{cyclic_groups}

\subsection{Symmetric groups}
($ S_{n}, \circ $) is a group.
\\\\
The size of any $ S_{n} $ is $ n! $
\\\\
The order of a $ k $-cycle  is $ k $.
\\\\
For any $ \sigma \in S_{n} $:
\begin{itemize}
\item for any $ i \in \{0...n\} $ there exists a $ d  > 0 $ such that $ \sigma^{d}(i) = i $ (i.e. $ \sigma^{d} \equiv Id = e $)
\item if $ d $ is the smallest integer such that $ \sigma^{d}(i) = i $ then the numbers $ i, \sigma^{1}(i), \sigma^{2}(i) ... \sigma^{d - 1}(i) $ are distinct
\item If $ j $ is not in the set $ \{i, \sigma(i), \sigma^{2}(i)...\sigma^{d - 1}(i)\} $ then neither is $ \sigma(j) $
\end{itemize}
Any permutation $ \sigma $ can be expressed as the product of disjoint $ k $-cycles.

\subsection{Subgroups}
Any group ($ G, \star $) has two trivial subgroups, ($ e, \star $) and itself.
\\\\
\textit{Subgroup test}
\\
Any $ H \subseteq G $ is a subgroup if:
\begin{itemize}
\item $ H \neq \emptyset $
\item for all $ x, y \in H, x \star y^{-1} \in H $
\end{itemize}

\subsection{Cosets and Lagrange's Theorem}
For any $ g_{1}, g_{2} \in G $ and subgroup $ H $:
\begin{equation}
g_{1}H = g_{2}H \Leftrightarrow g_{1} \in g_{2}H
\end{equation}
The left cosets of $ H $ form a partition of $ G $. This means any $ g \in G $ is in exactly one left coset of $ H $. The right cosets also form a (different) partition.
\\\\
For any $ g \in G $:
\begin{equation}
\#gH = \#hG = \#H
\end{equation}
\\
\textit{Lagrange's Theorem}
\\
For any subgroup ($ H, \star $) where $ H \subseteq G $:
\begin{equation}
\#G = \#H \cdot \#(G : H)
\end{equation}
\\
For any $ g \in G $, the order of $ g $ divides $ \#G $.
\\\\
If $ \#G = p $, where $ p $ is prime, then $ G $ is cyclic.

\newpage
\part{Analysis}

\section{Definitions}
\paragraph{Sequence}
A sequence is simply a map $ f: \N \to \R $, denoted by $ a_{n} $
\paragraph{Convergence (as $ n \to \infty $)}
A sequence $ a_{n} $ converges to a limit L if for all real numbers $ \epsilon > 0 $, there exists an $ N \in \N $ such that for all $ n > N $ we have $ |a_{n} - L| < \epsilon $.
\begin{equation}
\forall \epsilon > 0 \quad \exists N \in \N \quad s.t \quad \forall n > N \quad |a_{n} - L| < \epsilon
\end{equation}
\paragraph{Tends to infinity (sequence)}
We say a sequence tends to infinity if for all $ R \in \R $, the sequence $ a_{n} $ is eventually bigger than $ R $.
\begin{equation}
\forall R \in \R \quad \exists N \in \N \quad s.t. \quad \forall n > N \quad a_{n} > R
\end{equation}
\paragraph{Shift}
The shift of a sequence by say, k, is the sequence $ b_{n} = a_{n + k} $
\paragraph{Triangle inequality}
The general triangle inequality is:
\begin{equation}
|x - y| < |x - z| + |z - y|
\end{equation}
Setting $ z = 0 $ gives us:
\begin{align}
|x - y| & > |x| - |y|
\end{align}
Then setting $ y = - y $ gives us the familiar case:
\begin{align}
|x + y| & < |x| + |y|
\end{align}
\paragraph{Bounded above}
A sequence $ a_{n} $ is bounded above if there's a real number $ A $ such that $ a_{n} < A $ for all $ n $.
\paragraph{Bounded below}
A sequence $ a_{n} $ is bounded below if there's a real number $ A $ such that $ a_{n} > A $ for all $ n $.
\paragraph{Bounded}
A sequence $ a_{n} $ is bounded if there's a real number $ A $ such that $ |a_{n}| < A $ for all $ n $.
\paragraph{Increasing}
A sequence is increasing if $ a_{n + 1} \geq a_{n} $ for all n.
\paragraph{Strictly increasing}
A sequence is strictly increasing if $ a_{n + 1} > a_{n} $ for all n.
\paragraph{Decreasing}
A sequence is decreasing if $ a_{n + 1} \leq a_{n} $ for all n.
\paragraph{Strictly decreasing}
A sequence is strictly decreasing if $ a_{n + 1} < a_{n} $ for all n.
\paragraph{Monotonic}
A sequence is monotonic if it is increasing or decreasing.
\paragraph{Supremum}
The supremum A of a set $ S $ is the least upper bound of that set i.e. the smallest number such that $ s \leq A $ for all $ s \in S $.
\paragraph{Supremum (function)}
The supremum of a function $ f $ is the sup of $ \{f(x) \ | \ x \in \textrm{dom}(f)\} $.
\paragraph{Infimum}
The infimum B of a set $ S $ is the greatest lower bound of that set i.e. the largest number such that $ s \geq B $ for all $ s \in S $.
\paragraph{Infimum (function)}
The infimum of a function $ f $ is the inf of $ \{f(x) \ | \ x \in \textrm{dom}(f)\} $.
\paragraph{Subsequence}
A subsequence of $ a_{n} $ is a sequence $ a_{f(n)} $, where $ f(n) $ is a strictly increasing function.
\paragraph{Cauchy sequence}
A sequence is Cauchy if all the terms get arbitrarily close to one another. To put it mathematically:
\begin{equation}
\forall \epsilon > 0 \quad \exists N \in \N \quad s.t \quad \forall m,n \geq N \quad |a_{n} - a_{m}| < \epsilon
\end{equation}
\paragraph{Partial sum}
The $ n^{th} $ partial sum $ S_{n} $ of a sequence $ a_{n} $ is the sum of terms up to that point:
\begin{equation}
S_{n} = \sum_{i=1}^{n} a_{n}
\end{equation}
\paragraph{Summable}
A sequence is summable if the sequence of its partial sums converges. The limit of the sequence of partial sums will be:
\begin{equation}
L = \sum_{i=1}^{\infty} a_{n}
\end{equation}
\paragraph{Absolutely summable}
A sequence $ a_{n} $ is absolutely summable if $ |a_{n}| $ is summable.
\paragraph{Conditionally summable}
A sequence is conditionally summable if it is summable but not absolutely summable.
\paragraph{Power series}
The power series associated with a sequence $ a_{n} $ is the sequence of partial sums:
\begin{equation}
\sum_{i=1}^{n} a_{i}x^{i}
\end{equation}
\paragraph{Radius of convergence}
The radius of convergence R of a power series $ P(x) $ is defined as the largest $ x $ for which $ P(x) $ is convergent.
\begin{equation}
R = sup\{x \in \R \ | \ P(x) \textrm{ convergent}\}
\end{equation}
\paragraph{Limit as $ x \to \infty $ (function)}
A function $ f(x) $ tends to a limit $ L $ as $ x \to \infty $ if for all real numbers $ \epsilon > 0 $, there exists an $ R \in \R $ such that for all $ x \geq R $ we have $ |f(x) - L| < \epsilon $.
\begin{equation}
\forall \epsilon > 0 \quad \exists R \in \R \quad s.t \quad \forall x > R \quad |f(x) - L| < \epsilon
\end{equation}
\paragraph{Tends to infinity (function)}
A function $ f(x) $ tends to infinity as $ x \to \infty $ if for any $ M \in \R $ there exists an $ R \in \R $ such that if $ x > M $ then $ f(x) > R $.
\begin{equation}
\forall M \in \R \quad \exists R \in \R \quad s.t. \quad x > M \Rightarrow f(x) > R
\end{equation}
\paragraph{One-sided limit}
A function $ f(x) $ tends to a limit $ L $ as $ x \to a^{-} $ if for any $ \epsilon > 0 $ there exists a $ \delta > 0 $ such that if $ x \in (a - \delta, a) $ then $ |f(x) - L| < \epsilon $.
\begin{equation}
\forall \epsilon > 0 \quad \exists \delta > 0 \quad s.t. \quad x \in (a - \delta, a) \Rightarrow |f(x) - L| < \epsilon
\end{equation}
\noindent Same format for the other sided limit ($ x \to a^{+} $)
\\
(Note that $ \epsilon - \delta $ definition is only used for limits as x tends to a finite number a, not infinity)
\paragraph{Limit as $ x \to a $}
A function $ f(x) $ tends to a limit $ L $ as $ x \to a $ if we have both:
\begin{equation}
\lim_{x \to a^{-}}f(x) = L \quad \textrm{and} \quad \lim_{x \to a^{+}}f(x) = L
\end{equation}
\paragraph{Limit as $ x \to a $ ($ \epsilon $ - $ \delta $ def.)}
A function $ f(x) $ tends to a limit $ L $ as $ x \to a $ if:
\begin{equation}
\forall \epsilon > 0 \quad \exists \delta > 0 \quad s.t. \quad |x - a| < \delta \Rightarrow |f(x) - L| < \epsilon
\end{equation}
\paragraph{Continuous}
A function $ f(x) $ is continuous at a if:
\begin{equation}
\lim_{x \to a}f(x) = f(a)
\end{equation}
\paragraph{Continuous ($ \epsilon $ - $ \delta $ def.)}
A function $ f(x) $ is continuous at a if for all $ \epsilon > 0 $ there is a $ \delta > 0 $ such that if $ |x - a| < \delta $ then $ |f(x) - f(a)| < \epsilon $.
\begin{equation}
\forall \epsilon > 0 \quad \exists \delta > 0 \quad s.t. \quad |x - a| < \delta \Rightarrow |f(x) - f(a)| < \epsilon
\end{equation}
\paragraph{Continuous everywhere}
A function $ f(x) $ is continuous everywhere if it is continuous at a for all $ a \in \textrm{dom}(f) $.
\paragraph{Open interval}
An open inteval $ I $ is a set $ I \subseteq \R $ of the form:
\begin{itemize}
\item $ I = (a, b) $ for some $ a, b \in \R $, or
\item $ I = (-\infty, b) $, or
\item $ I = (a, +\infty) $, or
\item $ I = \R $
\end{itemize}
\paragraph{Discontinuity}
Discontinuity is the negation of continuity. Hence a function $ f(x) $ is discontinuous at a if there exists $ \epsilon > 0 $ such that for all $ \delta > 0 $, $ |x - a| < \delta $ AND $ |f(x) - f(a)| > \epsilon $.
\begin{equation}
\exists \epsilon > 0 \quad s.t. \quad \forall \delta > 0 \quad |x - a| < \delta \textrm{ AND } |f(x) - f(a)| > \epsilon
\end{equation}
\paragraph{Bounded (function)}
A function $ f(x) $ is bounded if the set of all possible values of f(x) is bounded.
\paragraph{Differentiable (ver. 1)}
A function $ f(x) $ is differentiable at a if:
\begin{equation}
\lim_{x \to a}\frac{f(x) - f(a)}{x - a}
\end{equation}
exists.
\paragraph{Differentiable (ver. 2)}
A function $ f(x) $ is differentiable at a if:
\begin{equation}
\lim_{h \to 0}\frac{f(a + h) - f(a)}{h}
\end{equation}
exists.
\paragraph{Differentiable everywhere}
A function $ f(x) $ is differentiable everywhere if it is differentiable at a for all $ a \in \textrm{dom}(f) $.
\paragraph{Global maximum}
A function $ f(x) $ has a global maximum at a if $ f(a) \geq f(x) $ for all other values of $ f(x) $.
\\\\
Similar definition for global minimum.
\paragraph{Local maximum}
A function $ f(x) $ has a local maximum at a if $ f(a) \geq f(x) $ for all x in the set $ (a - \epsilon, a + \epsilon) $, for some $ \epsilon $.
\\\\
Similar definition for local minimum.
\paragraph{Lipschitz continuous}
A function is Lipschitz continuous if:
\begin{equation}
|f'(x)| \leq L \Rightarrow |f(x_{1}) - f(x_{2})| \leq L|x_{1} - x_{2}|
\end{equation}

\section{Theorems}
\subsection{Sequences}
Every convergent sequence has a unique limit.
\\\\
Every convergent sequence is bounded.
\\\\
If all terms of a convergent sequence are larger than a number $ B $, then so is its limit.
\\\\
Some properties of limits:
\begin{align}
\lim_{x \to \infty}(a_{n} + b_{n}) & = \lim_{x \to \infty} a_{n} + \lim_{x \to \infty} b_{n} \\
\lim_{x \to \infty}(\lambda a_{n}) & = \lambda\lim_{x \to \infty} a_{n} \\
\lim_{x \to \infty}(a_{n}b_{n}) & = \lim_{x \to \infty} a_{n}\lim_{x \to \infty} b_{n} \\
\lim_{x \to \infty}(\frac{a_{n}}{b_{n}}) & = \frac{\lim_{x \to \infty} a_{n}}{\lim_{x \to \infty} b_{n}}
\end{align}
where $ \lambda $ is any real number.
\\\\
If $ a_{n} \to \infty $ and $ b_{n} $ is bounded below, $ a_{n} + b_{n} \to \infty $.
\\\\
If $ a_{n} \to \infty $ and $ b_{n} $ is bounded below by a positive number, $ a_{n}b_{n} \to \infty $.
\\\\
If $ a_{n} $ is bounded and $ b_{n} \to \infty $, then $ \frac{a_{n}}{b_{n}} \to 0 $.
\\\\
If $ a_{n} \to \infty $, for any real number $ \lambda $:
\begin{itemize}
\item $ \lambda < 0 \Rightarrow \lambda a_{n} \to -\infty $
\item $ \lambda = 0 \Rightarrow \lambda a_{n} \to 0 $
\item $ \lambda > 0 \Rightarrow \lambda a_{n} \to \infty $
\end{itemize}
If $ a_{n} \to a $ and $ b_{n} \to b $, and for all $ n $ $ a_{n} < b_{n} $, then $ a < b $.
\\\\
\textit{Sandwich Theorem}
\\
If $ a_{n} \leq b_{n} \leq c_{n} $ for all n, and $ a_{n} $ and $ c_{n} $ tend to the same limit $ L $, then $ b_{n} \to L $.
\\\\
Every bounded monotonic sequence is convergent.
\\\\
\textit{Completeness Axiom}
\\
Every non-empty subset of the real numbers which is bounded above has a supremum. Similar statement for infimum.
\\\\
Useful results for sequences:
\begin{align}
\lim_{n \to \infty} \lambda^{n} = 
\begin{cases}
\infty & \lambda > 1 \\
1 & \lambda = 1 \\
0 & -1 < \lambda < 1
\end{cases}
\end{align}
\begin{center}
$ \lambda^{n} $ diverges if $ \lambda = -1 $.
\end{center}
\begin{center}
If $ m > 0 $ and $ \lambda > 1 $ then $ \frac{\lambda^{n}}{n^{m}} \to \infty $ (exponentials beat powers).
\end{center}
\begin{center}
If $ m > 0 $ then $ \frac{\log(n)}{n^{m}} \to 0 $ (powers beat logs).
\end{center}

\subsection{Subsequences}
If $ a_{n} \to L $ then any subsequence $ a_{f(n)} \to L $.
\\\\
If two subsequences of $ a_{n} $ converge to different limits, $ a_{n} $ doesn't converge to a limit.
\\\\
Every sequence has a monotonic subsequence.
\\\\
\textit{Bolzano-Weierstrass Theorem}
\\
Every bounded sequence has a convergent subsequence.
\\\\
Every Cauchy sequence is bounded.
\\\\
Cauchy sequence $ \Leftrightarrow $ convergent sequence (for real numbers).
\\\\
\hspace*{-1in}
\includegraphics[scale=0.4]{sequences}

\subsection{Summability}
A sequence is summable iff the sequence of its partial sums converges.
\\\\
If two subsequences of a sequence $ a_{n} $ converge to two different limits, $ a_{n} $ is not summable.
\\\\
If $ a_{n} $ and $ b_{n} $ are summable with $ \sum_{i = 0}^{\infty} a_{i} = a $ and $ \sum_{i = 0}^{\infty} b_{i} = b $:
\begin{itemize}
\item $ a_{n} + b_{n} $ is summable with $ \sum_{i = 0}^{\infty} (a_{i} + b_{i}) = a + b $.
\item $ \lambda a_{n} $ is summable with $ \sum_{i = 0}^{\infty} \lambda a_{i} = \lambda a $ (for any real number $ \lambda $)
\end{itemize}
If $ b_{n} = a_{n + k} $ then $ a_{n} $ summable $ \Leftrightarrow b_{n} $ summable.
\\\\
$ a_{n} $ is summable $ \Rightarrow a_{n} \to 0 $.
\\\\
Let $ S_{n} $ denote the sequence of partial sums of $ a_{n} $ ($ S_{n} = \sum_{i = 0}^{n} a_{n}) $. A sequence of non-negative numbers $ a_{n} $ is summable iff $ S_{n} $ is bounded above. Similar statement for sequences of non-positive numbers.
\\\\
Every absolutely summable sequence is summable.
\\\\
\textit{Comparison test}
\\
If $ b_{n} > a_{n} $ for all $ n $ then $ b_{n} $ summable $ \Rightarrow a_{n} $ summable.
\\\\
\textit{Alternating series test}
\\
If $ a_{n} $ is a decreasing sequence AND $ a_{n} \geq 0 $ for all $ n $ AND $ a_{n} \to 0 $ then $ (-1)^{n+1} a_{n} $ is a convergent sequence.
\\\\
\textit{Ratio test for sequences}
\\
Let $ r = \lim_{n \to \infty} \frac{a_{n + 1}}{a_{n}} $:
\begin{itemize}
\item $ r < 1 \Rightarrow a_{n} $ is absolutely summable
\item $ r > 1 \Rightarrow a_{n} $ is not summable
\item $ r = 1 $ is an indeterminate case
\end{itemize}

\subsection{Power series}
The power series associated with a sequence $ a_{n} $ converges iff the sequence of partial sums  of $ a_{n}x^{n} $ converges (i.e. if $ \sum_{i = 0}^{n} a_{i}x^{i} $ converges).
\\\\
Let $ P(x) $ be a power series. If $ P(a) $ converges absolutely for some $ a $, then $ P(x) $ converges absolutely for all $ x $ such that $ |x| < |a| $
\\\\
Let $ R $ be the radius of convergence of $ P(x) $. For all real numbers $ a $:
\begin{itemize}
\item $ |a| < R \Rightarrow P(a) $ converges absolutely
\item $ |a| > R \Rightarrow P(a) $ diverges
\end{itemize}
\textit{Ratio test for power series}
\\
Let $ r = \frac{a_{n + 1}}{a_{n}} $. Let $ P(x) = \sum_{i = 0}^{n} a_{i}x^{i} $ (i.e. the power series associated with $ a_{n} $):
\begin{itemize}
\item $ r \to 0 \Rightarrow R = \infty $
\item $ r \to L $ for some $ L \Rightarrow R = \frac{1}{L} $
\item $ r \to \infty \Rightarrow R = 0 $
\end{itemize}
Note: if $ r = 1 $ here then $ R = 1 $. This is DIFFERENT to the ratio test for sequences, where $ r = 1 $ is an indeterminate case.

\subsection{Continuity}
The limit of a function at any specific point is unique.
\\\\
If functions $ f $ and $ g $ are continuous at a:
\begin{itemize}
\item $ (f + g) $ is continuous at a
\item $ fg $ is continuous at a
\item $ \frac{1}{f(x)} and \frac{1}{g(x)} $ are continuous at a
\item $ g \circ f $ is continuous at a
\end{itemize}
Any polynomial in $ \R $ is continuous
\\\\
Any rational function in $ \R $ is continuous
\\\\
\textit{Sequential continuity}
\\
A function $ f $ is continuous at $ a $ iff $ f(a_{n}) \to f(a) $ for all sequences $ a_{n} $ such that $ a_{n} \to a $.
\\\\
Any continuous function on a closed bounded interval is bounded.
\\\\
\textit{Intermediate Value Theorem}
\\
If $ f $ continuous and $ f(a) \leq f(b) $ for some $ a, b $, then there exists some $ c \in [a, b] $ such that $ f(a) \leq f(c) \leq f(b) $.
\\\\
\textit{Fixed Point Theorem}
\\
If $ f $ continuous and $ f: [a, b] \to [a, b] $, then there exists some $ c \in [a, b] $ such that $ f(c) = c $.
\\\\
Polynomials of odd degree have at least 1 root.
\\\\
$ f $ differentiable $ \Rightarrow f $ continuous.
\subsection{Differentiable functions}
If functions $ f $ and $ g $ are differentiable at a:
\begin{itemize}
\item $ (f + g) $ is differentiable at a
\item $ fg $ is differentiable at a
\item $ \frac{1}{f(x)} and \frac{1}{g(x)} $ are differentiable at a
\item $ g \circ f $ is differentiable at a
\item $ g^{-1} $ and $ f^{-1} $ are differentiable at a
\end{itemize}
Let $ f $ be continuous and differentiable. If $ f $ has a local extremum at $ a $ then $ f'(a) = 0 $ (except at endpoints of the interval).
\\\\
Let $ f $ be continuous and differentiable. If $ f $ has a local extremum at $ c $ (say in the interval $ [a, b] $), there are 3 possiblities:
\begin{itemize}
\item $ c $ is an endpoint of $ [a, b] $
\item $ f'(c) = 0 $
\item $ c $ is a non-differentiable point
\end{itemize}
\textit{Mean Value Theorem}
\\
Let $ f $ be continuous on $ [a, b] $ and differentiable on $ (a, b) $. There exists a point $ c \in (a, b) $ such that:
\begin{equation}
f'(c) = \frac{f(b) - f(a)}{b - a}
\end{equation}
\\\\
\textit{Rolle's Theorem}
\\
Let $ f $ be continuous and differentiable on $ (a, b) $. If $ f(a) = f(b) $ then there exists some $ c \in (a, b) $ such that $ f'(c) = 0 $. This is a special case of the Mean Value Theorem.

\end{document}